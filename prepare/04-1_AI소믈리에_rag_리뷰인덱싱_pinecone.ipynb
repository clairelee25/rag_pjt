{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb338003-1bf9-4382-a5fd-ac6cc184bce3",
   "metadata": {},
   "source": [
    "# 와인 리뷰데이터 인덱싱\n",
    "- 벡터 DB : pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8dabfc6-8798-4757-9392-f3a68014448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load_dotenv(dotenv_path=\"../.env\")\n",
    "load_dotenv(override=True, dotenv_path=\"../.env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_NAMESPACE = os.getenv(\"PINECONE_NAMESPACE\")\n",
    "# PINECONE_NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358dbe6-a1ba-484f-bd56-72df457689a3",
   "metadata": {},
   "source": [
    "# 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58dd0cd9-36dd-442e-b538-2e182e36429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import os\n",
    "\n",
    "csv_path = os.path.abspath(\"wine_reviews/winemag-data-130k-v2.csv\")\n",
    "loader = CSVLoader(csv_path, encoding=\"latin-1\")   # 또는 \"ISO-8859-1\")\n",
    "docs = loader.load()\n",
    "\n",
    "# \"prepare/wine_reviews/winemag-data-130k-v2.csv\"\n",
    "# loader = CSVLoader('./wine_reviews/winemag-data-130k-v2.csv')\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d850e70-b01d-4cdb-9d55-643ee5dc319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129971"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632eaad2-b69a-4350-ab43-b5d10d9cfbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding 모델 객체 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0009ceb-6548-4590-8c5d-fee226f02629",
   "metadata": {},
   "source": [
    "# Pinecone객체, index객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa26442f-a169-4697-93ed-872be3515814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Pinecone 클라이언트를 초기화(객체생성)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0f6ace-2180-4d85-9bce-8a9bed25928c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'wine-reviews' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# pinecone에 index list 가져오기\n",
    "existing_indexes = pc.list_indexes()\n",
    "\n",
    "# 이름만 추출\n",
    "index_names = [index['name'] for index in existing_indexes.indexes]\n",
    "# print(index_names)\n",
    "\n",
    "# index 이름이 존재 하지 않으면 생성\n",
    "if PINECONE_INDEX_NAME not in index_names:\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=1536,  # 모델 차원, openapi embeding model을 사용함. 정확하게 일치\n",
    "        metric=\"cosine\",  # 모델 메트릭, openapi embeding model 에서 사용하는 것 확인\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    print(f\"Index '{PINECONE_INDEX_NAME}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{PINECONE_INDEX_NAME}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f9da98-f89c-4915-9039-2ac9346ec6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split하기\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 설정 (예: 1000자씩 분할)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100,\n",
    "    # length_function=tiktoken_len,  # 토큰 기반 길이 측정    \n",
    "    length_function=len,  # 문자수   \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "# 문서를 분할\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c220facf-56eb-43b2-8c2d-842a1ce6dfad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 첫 번째 배치로 벡터 스토어 생성\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     vector_store = \u001b[43mPineconeVectorStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# BATCH_SIZE 수 만큼의 chunk\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 임베딩 벡터로 변환\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPINECONE_INDEX_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# index 이름\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPINECONE_NAMESPACE\u001b[49m\u001b[43m      \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 이후 배치는 생성한 벡터 스토어에 추가, # 내부적으로 임베딩 벡터로 변환\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     20\u001b[39m     vector_store.add_documents(batch_docs)    \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:813\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    810\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    811\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:978\u001b[39m, in \u001b[36mPineconeVectorStore.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, async_req, id_prefix, **kwargs)\u001b[39m\n\u001b[32m    975\u001b[39m pinecone_index = \u001b[38;5;28mcls\u001b[39m.get_pinecone_index(index_name, pool_threads)\n\u001b[32m    976\u001b[39m pinecone = \u001b[38;5;28mcls\u001b[39m(pinecone_index, embedding, text_key, namespace, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[43mpinecone\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_chunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_req\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsert_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pinecone\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:357\u001b[39m, in \u001b[36mPineconeVectorStore.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, id_prefix, **kwargs)\u001b[39m\n\u001b[32m    355\u001b[39m chunk_ids = ids[i : i + embedding_chunk_size]\n\u001b[32m    356\u001b[39m chunk_metadatas = metadatas[i : i + embedding_chunk_size]\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m vector_tuples = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chunk_ids, embeddings, chunk_metadatas))\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[32m    360\u001b[39m     \u001b[38;5;66;03m# Runs the pinecone upsert asynchronously.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:709\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    708\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:576\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    575\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    578\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\lc_env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# vector sotre에 저장(2시간정도 걸림)\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "BATCH_SIZE = 500  # 한 번에 처리할 문서 수(최대 vector 수 1000개, 2MB 이내)\n",
    "\n",
    "for i in range(0, len(chunks), BATCH_SIZE):\n",
    "    batch_docs = chunks[i:i+BATCH_SIZE]\n",
    "    \n",
    "    # 첫 번째 배치로 벡터 스토어 생성\n",
    "    if i == 0:\n",
    "        vector_store = PineconeVectorStore.from_documents(\n",
    "            batch_docs,            # BATCH_SIZE 수 만큼의 chunk\n",
    "            embedding=embeddings,  # 임베딩 벡터로 변환\n",
    "            index_name=PINECONE_INDEX_NAME,   # index 이름\n",
    "            namespace=PINECONE_NAMESPACE      \n",
    "        )\n",
    "\n",
    "    # 이후 배치는 생성한 벡터 스토어에 추가, # 내부적으로 임베딩 벡터로 변환\n",
    "    else:\n",
    "        vector_store.add_documents(batch_docs)    \n",
    "    \n",
    "    print(f\"배치 {i//BATCH_SIZE + 1} 완료: {len(batch_docs)}개 문서 업로드\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9972f54-af48-403c-ae05-faea7b38815e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
